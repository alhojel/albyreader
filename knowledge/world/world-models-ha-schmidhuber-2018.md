# World Models

**Authors:** David Ha, Jurgen Schmidhuber
**Paper:** [arxiv.org/abs/1803.10122](https://arxiv.org/abs/1803.10122) (March 2018)
**Interactive version:** [worldmodels.github.io](https://worldmodels.github.io/)
**Published:** NeurIPS 2018 (Advances in Neural Information Processing Systems 31, pp. 2451-2463)

---

## Abstract

We explore building a generative neural network model of a popular reinforcement learning environment. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to the controller, we can train a compact and minimal controller to solve the required task. We can even train our agent entirely inside of its own hallucinated dream environment generated by the learned world model, and transfer this policy back into the actual environment.

---

## Core Concept

Can an agent learn a model of its world and then use that model to learn behavior entirely within its own "dream"? The paper decomposes the agent into three components:

```
Observation -> V (Vision) -> z_t -> M (Memory) -> h_t -> C (Controller) -> Action
```

### V — Vision Model (VAE)

A Variational Autoencoder that compresses high-dimensional observations into compact latent vectors.

- Convolutional encoder/decoder with 4 layers each
- Stride of 2, ReLU activations
- Output: 32-dimensional latent vector z (CarRacing) or 64-dim (VizDoom)
- ~4.3M parameters
- Learns to reconstruct frames from compact representation

### M — Memory Model (MDN-RNN)

An LSTM combined with a Mixture Density Network that predicts future latent states probabilistically.

- LSTM: 256 hidden units (CarRacing), 512 (VizDoom)
- Mixture Density Network with 5 Gaussian mixture components
- Models: P(z_{t+1}, d_{t+1} | a_t, z_t, h_t)
  - z_{t+1}: predicted next latent state
  - d_{t+1}: predicted probability of episode termination (VizDoom)
  - a_t: current action
  - h_t: hidden state
- ~422K parameters (CarRacing), ~1.6M (VizDoom)
- Can generate multiple possible futures by sampling from the mixture

### C — Controller

A deliberately minimal single linear layer that maps concatenated latent state and hidden state to actions.

- a_t = W_c [z_t ; h_t] + b_c
- 867 parameters (CarRacing), 1,088 (VizDoom)
- Trained via CMA-ES (Covariance Matrix Adaptation Evolution Strategy), not backpropagation
- Intentionally tiny — forces V and M to learn useful representations

---

## Experiments

### CarRacing-v0

Continuous control task: drive a car around randomly generated tracks. Score = tiles visited minus time penalty.

| Agent | Score |
|-------|-------|
| V model only (no memory) | 632 ± 251 |
| V + C (no hidden state input) | 788 ± 141 |
| Full V + M + C | **906 ± 21** |
| Previous SOTA | 838 ± 11 |

The memory component M was critical — it predicts upcoming road curvature, allowing the controller to anticipate turns.

### VizDoom: Take Cover

Dodge fireballs thrown by monsters. Objective: survive as long as possible.

**Key experiment:** Training entirely inside the dream (hallucinated environment generated by M).

| Setting | Score |
|---------|-------|
| Random baseline | 210 ± 108 |
| Trained in dream (τ=1.0) | 824 ± 58 |
| Trained in dream (τ=1.15) | **1092 ± 556** |
| Trained in actual environment | 868 ± 511 |

The agent trained in hallucination transferred successfully to the real environment and actually scored higher than one trained directly in it.

---

## The Temperature Trick

A critical technical contribution. When training in dreams, the agent can exploit imperfections in the learned world model — finding "cheats" that work in the dream but not reality.

The solution: add a temperature parameter τ controlling the stochasticity of the MDN-RNN's predictions during dream training.

- τ = 1.0: standard sampling (model's learned distribution)
- τ > 1.0: increased randomness, making the dream environment harder and more unpredictable
- τ < 1.0: reduced randomness, more deterministic dreams

Optimal transfer occurred at **τ ≈ 1.15** — slightly harder than reality. This forced the agent to develop more robust policies that generalized better.

This is analogous to how humans who practice in slightly more challenging conditions often perform better in actual conditions.

---

## Connection to Schmidhuber's Earlier Work

The paper builds on a long lineage of Schmidhuber's work on:

- **RNN-based world models (1990):** Using RNNs to learn environment dynamics for planning
- **Compressed network search:** Learning to compress observations and predict futures
- **Intrinsic motivation / curiosity:** Agents that explore to improve their world models
- **PowerPlay (2011):** Self-inventing problem-solving agents

The key new ingredients in 2018: modern VAEs for vision, modern LSTM+MDN for memory, and evolutionary strategies for training the compact controller — making the old ideas practical at scale.

---

## Design Philosophy

The architecture mirrors a theory of human cognition:

> "We humans develop a mental model of the world based on what we are able to perceive with our limited senses. The decisions and actions we make are based on this internal model."

The agent:
1. Compresses raw sensory data into abstract features (V — like visual cortex)
2. Builds a predictive model of temporal dynamics (M — like hippocampal memory)
3. Makes decisions based on these abstract representations (C — like prefrontal cortex)

The minimal controller (867 params!) demonstrates that if V and M learn good representations, very little "intelligence" is needed in the decision-making layer.

---

## References

The paper cites heavily from RL, generative models, and Schmidhuber's historical work. Notable references:

1. [Kingma & Welling (2013) — Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) — VAE foundational work
2. [Graves (2013) — Generating Sequences with RNNs](https://arxiv.org/abs/1308.0850)
3. [Schmidhuber (1990) — Making the World Differentiable](http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw.pdf) — original RNN world model concept
4. [Schmidhuber (1991) — Curious Model-Building Control Systems](http://www.idsia.ch/~juergen/curioussingapore/curioussingapore.html) — intrinsic motivation
5. [Schmidhuber (2015) — On Learning to Think](https://arxiv.org/abs/1511.09249) — RNN-based world models for planning
6. [Bishop (1994) — Mixture Density Networks](https://publications.aston.ac.uk/id/eprint/373/) — MDN foundational paper
7. [Salimans et al. (2017) — Evolution Strategies as Scalable Alternative to RL](https://arxiv.org/abs/1703.03864)
8. [Hansen (2016) — CMA-ES](https://arxiv.org/abs/1604.00772) — the controller training method
9. [Chiappa et al. (2017) — Recurrent Environment Simulators](https://arxiv.org/abs/1704.02254)
10. [Gemici et al. (2017) — Generative Temporal Models with Memory](https://arxiv.org/abs/1702.04649)
11. [Hinton & Nair (2006) — Restricted Boltzmann Machines for Collaborative Filtering](https://www.cs.toronto.edu/~hinton/absps/netflixICML.pdf)
12. [Hochreiter & Schmidhuber (1997) — Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) — the LSTM paper
13. [Sutskever & Hinton (2007) — Learning Multilevel Distributed Representations](https://papers.nips.cc/paper/3048-learning-multilevel-distributed-representations-for-high-dimensional-sequences)
14. [Hafner et al. (2018) — Learning Latent Dynamics for Planning (PlaNet)](https://arxiv.org/abs/1811.04551)
15. [Wayne et al. (2018) — Unsupervised Predictive Memory in Goal-Directed Agent (MERLIN)](https://arxiv.org/abs/1803.10760)
16. [Pathak et al. (2017) — Curiosity-driven Exploration](https://arxiv.org/abs/1705.05363)
17. [Denton & Fergus (2018) — Stochastic Video Generation](https://arxiv.org/abs/1802.07687)
18. [Oh et al. (2015) — Action-Conditional Video Prediction (Atari)](https://arxiv.org/abs/1507.08750)
19. [Fragkiadaki et al. (2016) — Learning Visual Predictive Models](https://arxiv.org/abs/1605.07157)
20. [Leibfried et al. (2017) — Model-based RL with Stochastic World Models](https://arxiv.org/abs/1709.02702)
